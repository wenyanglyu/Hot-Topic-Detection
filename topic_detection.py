# -*- coding: utf-8 -*-
"""Topic Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m1lXYo8eZPRmBuT3rEpfpdNpRladN9yo

# READ ME

---


## Part 1: Unzip and Segment Files into different demography

## Part 2: Pre-process dataset per demography

## Part 3: Determine the topics and visualize the results per demography

## Part 4: Counting Nouns, TFIDF, nGram and LDA with TFIDF

## Part 5: Test best topic number setting for LDA with count

## Part 6: Test best topic number setting for NMF with TFIDF

Paper link:

https://drive.google.com/file/d/18XRMoG7VTBbdmwt8BcF6V3llrVjKXygv/view?usp=drive_link

---
© wenyang and Henry

# **Part 1: Dataset Preparation**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## 1.1 Load and Unzip Files
Files are saved in temp variable extract_path
"""

import zipfile
import os

# Define paths
zip_path = '/content/drive/MyDrive/COMP814Data/Assignment2BlogData.zip'
# unzip into a temporary file path in collab
extract_path = '/content/extracted_files'

# Unzip the files
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Count the number of extracted files
file_count = sum([len(files) for r, d, files in os.walk(extract_path)])
print(f"Number of extracted files: {file_count}")

"""## 1.2 Define The Function to segment files into the required demographics

Files segmented are saved in temporary variables in collab
"""

import os
import re
from collections import defaultdict

def process_files_with_filter(extract_path, filter_keyword, batch_size):
    # Temporary variables to store file information
    file_info = []
    categorized_files = defaultdict(list)  # Store files based on categories

    # Create a dictionary to count the number of files in each category
    category_counts = defaultdict(int)

    # Get the list of all files
    for root, dirs, files in os.walk(extract_path):
        total_files = len(files)

        for i in range(0, total_files, batch_size):
            batch_files = files[i:i + batch_size]
            print(f"Processing batch {i // batch_size + 1} with {len(batch_files)} files.")

            # Process the current batch
            for file in batch_files:
                file_path = os.path.join(root, file)
                if file.endswith('.xml'):
                    age_match = re.search(r'\.(\d+)\.', file)
                    if age_match:
                        age = int(age_match.group(1))
                    else:
                        age = None
                    file_info.append({
                        'filename': file,
                        'filepath': file_path,
                        'age': age
                    })

            # Process files based on filter keyword
            for info in file_info:
                file = info['filename']
                file_path = info['filepath']
                age = info['age']

                if filter_keyword == 'male' and '.male.' in file.lower():
                    category_counts["male"] += 1
                    categorized_files["male"].append(info)

                elif filter_keyword == 'female' and '.female.' in file.lower():
                    category_counts["female"] += 1
                    categorized_files["female"].append(info)

                elif filter_keyword == 'student' and '.student.' in file.lower():
                    category_counts["student"] += 1
                    categorized_files["student"].append(info)

                elif filter_keyword == 'ageOlder' and age is not None and age > 20:
                    category_counts["ageOlder"] += 1
                    categorized_files["ageOlder"].append(info)

                elif filter_keyword == 'ageYoung' and age is not None and age <= 20:
                    category_counts["ageYoung"] += 1
                    categorized_files["ageYoung"].append(info)

                elif filter_keyword == 'everyone':
                    category_counts["everyone"] += 1
                    categorized_files["everyone"].append(info)

            # Clear file_info for next batch to free up memory
            file_info.clear()




    # Print the number of files in each category
    for category, count in category_counts.items():
        print(f"\nNumber of files in {category}: {count}")

    return categorized_files

"""## 1.3 Segment Files into different variables"""

# Process and store files for each category
maleFiles = process_files_with_filter(extract_path, 'male', 1000)
femaleFiles = process_files_with_filter(extract_path, 'female', 1000)
studentFiles = process_files_with_filter(extract_path, 'student', 1000)
ageOlderFiles = process_files_with_filter(extract_path, 'ageOlder', 1000)
ageYoungFiles = process_files_with_filter(extract_path, 'ageYoung', 1000)
everyoneFiles = process_files_with_filter(extract_path, 'everyone', 1000)

"""# Part 2: Pre-Processing

## 2.1 Install all packages
"""

pip install unidecode pyspellchecker dask[distributed] pyLDAvis gensim scikit-learn matplotlib seaborn textblob

import textblob.download_corpora as download_corpora
download_corpora.download_all()

"""## 2.2 Define the pre-processing function

Pre-processing with dask parallel mode; use processingNumber to control how many files are processed, default setting is all files being processed
"""

import nltk
import string
import re
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from unidecode import unidecode
from spellchecker import SpellChecker
from dask import delayed, compute
from dask.distributed import Client

# Ensure the required resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load spaCy English model
nlp = spacy.load('en_core_web_sm')

# Initialize spell checker
spell = SpellChecker()


processingNumber = 99999

# Define additional stopwords
additional_stopwords = set([
    'blog', 'date', 'post', '/blog', '/post','/data', 'comment', 'title', 'author'
])

# Combine nltk stopwords with additional stopwords
stop_words = set(stopwords.words('english')).union(additional_stopwords)

# Define additional punctuation and unwanted characters to remove
additional_punctuation = set([
    '``', "''", '...', '--', '``', "''", '/', '\\'
])

# Combine string punctuation with additional punctuation
all_punctuation = set(string.punctuation).union(additional_punctuation)

# Define a list of month names to remove
months = [
    'january', 'february', 'march', 'april', 'may', 'june',
    'july', 'august', 'september', 'october', 'november', 'december'
]

monthsBug = [month + ',' for month in months]

def is_abnormal_token(token):
    if re.match(r'^[\.\-_=]+$', token):
        return True
    if re.match(r'^[\w\.\-_=]*[\W_]+[\w\.\-_=]*$', token):
        return True
    if len(token) == 1 and token in string.punctuation:
        return True
    return False

def split_into_sentences(text):
    sentences = re.split(r'(?<=[.!?]) +|\n', text)
    return sentences

def preprocess_text(text):
    # Lowercase the text
    text = text.lower()

    # Replace non-ASCII characters
    text = unidecode(text)

    for punct in additional_punctuation:
        text = text.replace(punct, ' ')

    for monthBug in monthsBug:
        text = text.replace(monthBug, ' ')

    # Tokenize
    tokens = word_tokenize(text)

    # Split tokens that are combinations of letters and digits or contain punctuation
    tokens = [re.split(r'[\W_]+', token) for token in tokens]
    tokens = [item for sublist in tokens for item in sublist if item]

    # Remove stop words
    tokens = [word for word in tokens if word.lower() not in stop_words]

    # Remove months
    tokens = [word for word in tokens if word.lower() not in months]

    # Remove abnormal tokens
    tokens = [word for word in tokens if not is_abnormal_token(word)]

    # Spell check and remove misspelled words and single characters
    tokens = [word for word in tokens if len(word) > 1 and spell.unknown([word]) == set()]

    # Remove numbers
    tokens = [word for word in tokens if not word.isdigit()]

    # Remove punctuation
    tokens = [word for word in tokens if word not in string.punctuation]

    # Stemming
    ps = PorterStemmer()
    tokens = [ps.stem(word) for word in tokens]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back to string
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text

@delayed
def preprocess_batch(batch_files):
    preprocessed_sentences = []
    for file_info in batch_files:
        try:
            with open(file_info['filepath'], 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(file_info['filepath'], 'r', encoding='latin-1') as f:
                content = f.read()

        # Split file content into sentences
        sentences = split_into_sentences(content)

        # Preprocess each sentence
        preprocessed_batch_sentences = [preprocess_text(sentence) for sentence in sentences]

        # Remove empty strings from preprocessed sentences
        preprocessed_batch_sentences = [sentence for sentence in preprocessed_batch_sentences if sentence]

        preprocessed_sentences.append(preprocessed_batch_sentences)

    return preprocessed_sentences

def preprocess_files_dask(categorized_files, batch_size):
    preprocessed_data = {}  # Dictionary to store preprocessed content
    for category, files in categorized_files.items():
        total_files = min(len(files), processingNumber)
        print(total_files)
        batches = [files[i:i + batch_size] for i in range(0, total_files, batch_size)]
        delayed_results = [preprocess_batch(batch) for batch in batches]
        preprocessed_batches = compute(*delayed_results, scheduler='processes')
        preprocessed_data[category] = [item for sublist in preprocessed_batches for item in sublist]
    print(len(preprocessed_data[category]))
    print(preprocessed_data[category][0])
    return preprocessed_data

"""## 2.3  Select which demographic dataset to process

Running Time with CPU on 4th June:

Students 5120   30 minutes

Male 9660   44 minutes

Female 9660   50 minutes

AgeOlder 11080  61 minutes

Age ≤ 20 8240   33 minutes

Everyone 19320     60 minutes

### 2.3.1 Pre-process Students File
"""

import time

batch_size = 200  # Define the batch size
# Start a Dask client
client = Client()

# Measure the time before processing
start_time = time.time()

studentProcessedData = preprocess_files_dask(studentFiles, batch_size)
# maleProcessedData = preprocess_files_dask(maleFiles, batch_size)
# femaleProcessedData = preprocess_files_dask(femaleFiles, batch_size)
# ageOlderProcessedData = preprocess_files_dask(ageOlderFiles, batch_size)
# ageYoungProcessedData = preprocess_files_dask(ageYoungFiles, batch_size)
# everyoneProcessedData = preprocess_files_dask(everyoneFiles, batch_size)

# Measure the time after processing
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
print(f"Processing time: {elapsed_time:.2f} seconds")

print(len(studentProcessedData['student']))
print(studentProcessedData['student'][1])
print(len(studentFiles['student']))
print(studentFiles['student'][0])

# Shut down the client when done
client.shutdown()

"""### 2.3.2 Pre-process Male File"""

import time

batch_size = 200  # Define the batch size
# Start a Dask client
client = Client()

# Measure the time before processing
start_time = time.time()


maleProcessedData = preprocess_files_dask(maleFiles, batch_size)


# Measure the time after processing
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
print(f"Processing time: {elapsed_time:.2f} seconds")

print(len(maleProcessedData['male']))
print(maleProcessedData['male'][1])
print(len(maleProcessedData['male']))
print(maleProcessedData['male'][0])

# Shut down the client when done
client.shutdown()

"""### 2.3.3 Pre-process Female File"""

import time

batch_size = 200  # Define the batch size
# Start a Dask client
client = Client()

# Measure the time before processing
start_time = time.time()


femaleProcessedData = preprocess_files_dask(femaleFiles, batch_size)


# Measure the time after processing
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
print(f"Processing time: {elapsed_time:.2f} seconds")

print(len(femaleProcessedData['female']))
print(femaleProcessedData['female'][1])
print(len(femaleProcessedData['female']))
print(femaleProcessedData['female'][0])

# Shut down the client when done
client.shutdown()

"""### 2.3.4 Pre-process ageOlder File"""

import time

batch_size = 200  # Define the batch size
# Start a Dask client
client = Client()

# Measure the time before processing
start_time = time.time()


ageOlderProcessedData = preprocess_files_dask(ageOlderFiles, batch_size)


# Measure the time after processing
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
print(f"Processing time: {elapsed_time:.2f} seconds")

print(len(ageOlderProcessedData['ageOlder']))
print(ageOlderProcessedData['ageOlder'][1])
print(len(ageOlderProcessedData['ageOlder']))
print(ageOlderProcessedData['ageOlder'][0])

# Shut down the client when done
client.shutdown()

"""### 2.3.5 Pre-process ageYoung File"""

import time

batch_size = 200  # Define the batch size
# Start a Dask client
client = Client()

# Measure the time before processing
start_time = time.time()

ageYoungProcessedData = preprocess_files_dask(ageYoungFiles, batch_size)


# Measure the time after processing
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
print(f"Processing time: {elapsed_time:.2f} seconds")

print(len(ageYoungProcessedData['ageYoung']))
print(ageYoungProcessedData['ageYoung'][1])
print(len(ageYoungProcessedData['ageYoung']))
print(ageYoungProcessedData['ageYoung'][0])

# Shut down the client when done
client.shutdown()

"""### 2.3.6 Pre-process everyone File"""

import time

batch_size = 200  # Define the batch size
# Start a Dask client
client = Client()

# Measure the time before processing
start_time = time.time()


everyoneProcessedData = preprocess_files_dask(everyoneFiles, batch_size)

# Measure the time after processing
end_time = time.time()

# Calculate the elapsed time
elapsed_time = end_time - start_time
print(f"Processing time: {elapsed_time:.2f} seconds")

print(len(everyoneProcessedData['everyone']))
print(everyoneProcessedData['everyone'][1])
print(len(everyoneProcessedData['everyone']))
print(everyoneProcessedData['everyone'][0])

# Shut down the client when done
client.shutdown()

"""# Part 3: NMF with tfidfVec
determine the two most dominant topics

draw the wordcloud

extract the top10 text most related to per dominant topic

## 3.1 Students Topics
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 15
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_student = copy.deepcopy(studentProcessedData)
print(f"Student data samples: {len(processing_data_student)}")


# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_student.values())
print(f"Total number of files in the dataset: {total_files}")

random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_student.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot word clouds for the top 2 dominant topics
plt.figure(figsize=(10, 5), dpi=128)

for i, topic_idx in enumerate(top_topic_indices):
    ax = plt.subplot(1, 2, i + 1)
    word_freq = {feature_names[j]: H[topic_idx, j] for j in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Extract the top 10 documents most related to each dominant topic
def get_top_documents(W, topic_num, top_n=10):
    topic_probabilities = [(i, prob) for i, prob in enumerate(W[:, topic_num])]
    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    return top_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top documents
for topic_num in top_topic_indices:
    top_documents = get_top_documents(W, topic_num, top_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

# Plot the topic distribution for the top 10 documents related to the most dominant topic
most_dominant_topic = top_topic_indices[0]
top_documents = get_top_documents(W, most_dominant_topic, top_n=10)
top_document_indices = [doc_index for doc_index, prob in top_documents]

plt.figure(figsize=(10, 6))
for doc_index in top_document_indices:
    plt.plot(W[doc_index], label=f'Doc {doc_index}')

plt.xlabel('Topic')
plt.ylabel('Topic Weight')
plt.title(f'Topic Distribution for Top 10 Documents Related to Topic {most_dominant_topic}')
plt.legend(loc='upper right')
plt.show()

"""## 3.2 Male Topics"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 15
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing


processing_data_male = copy.deepcopy(maleProcessedData)
print(f"Male data samples: {len(processing_data_male)}")

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_male.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_male.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot word clouds for the top 2 dominant topics
plt.figure(figsize=(10, 5), dpi=128)

for i, topic_idx in enumerate(top_topic_indices):
    ax = plt.subplot(1, 2, i + 1)
    word_freq = {feature_names[j]: H[topic_idx, j] for j in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Extract the top 10 documents most related to each dominant topic
def get_top_documents(W, topic_num, top_n=10):
    topic_probabilities = [(i, prob) for i, prob in enumerate(W[:, topic_num])]
    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    return top_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top documents
for topic_num in top_topic_indices:
    top_documents = get_top_documents(W, topic_num, top_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

# Plot the topic distribution for the top 10 documents related to the most dominant topic
most_dominant_topic = top_topic_indices[0]
top_documents = get_top_documents(W, most_dominant_topic, top_n=10)
top_document_indices = [doc_index for doc_index, prob in top_documents]

plt.figure(figsize=(10, 6))
for doc_index in top_document_indices:
    plt.plot(W[doc_index], label=f'Doc {doc_index}')

plt.xlabel('Topic')
plt.ylabel('Topic Weight')
plt.title(f'Topic Distribution for Top 10 Documents Related to Topic {most_dominant_topic}')
plt.legend(loc='upper right')
plt.show()

"""## 3.3 Female Topics"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 15
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing

processing_data_female = copy.deepcopy(femaleProcessedData)
print(f"Female data samples: {len(processing_data_female)}")

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_female.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_female.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot word clouds for the top 2 dominant topics
plt.figure(figsize=(10, 5), dpi=128)

for i, topic_idx in enumerate(top_topic_indices):
    ax = plt.subplot(1, 2, i + 1)
    word_freq = {feature_names[j]: H[topic_idx, j] for j in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Extract the top 10 documents most related to each dominant topic
def get_top_documents(W, topic_num, top_n=10):
    topic_probabilities = [(i, prob) for i, prob in enumerate(W[:, topic_num])]
    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    return top_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top documents
for topic_num in top_topic_indices:
    top_documents = get_top_documents(W, topic_num, top_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

# Plot the topic distribution for the top 10 documents related to the most dominant topic
most_dominant_topic = top_topic_indices[0]
top_documents = get_top_documents(W, most_dominant_topic, top_n=10)
top_document_indices = [doc_index for doc_index, prob in top_documents]

plt.figure(figsize=(10, 6))
for doc_index in top_document_indices:
    plt.plot(W[doc_index], label=f'Doc {doc_index}')

plt.xlabel('Topic')
plt.ylabel('Topic Weight')
plt.title(f'Topic Distribution for Top 10 Documents Related to Topic {most_dominant_topic}')
plt.legend(loc='upper right')
plt.show()

"""## 3.4 ageOlder Topics"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 15
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing

processing_data_age_older = copy.deepcopy(ageOlderProcessedData)
print(f"Age older data samples: {len(processing_data_age_older)}")

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_age_older.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_age_older.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot word clouds for the top 2 dominant topics
plt.figure(figsize=(10, 5), dpi=128)

for i, topic_idx in enumerate(top_topic_indices):
    ax = plt.subplot(1, 2, i + 1)
    word_freq = {feature_names[j]: H[topic_idx, j] for j in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Extract the top 10 documents most related to each dominant topic
def get_top_documents(W, topic_num, top_n=10):
    topic_probabilities = [(i, prob) for i, prob in enumerate(W[:, topic_num])]
    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    return top_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top documents
for topic_num in top_topic_indices:
    top_documents = get_top_documents(W, topic_num, top_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

# Plot the topic distribution for the top 10 documents related to the most dominant topic
most_dominant_topic = top_topic_indices[0]
top_documents = get_top_documents(W, most_dominant_topic, top_n=10)
top_document_indices = [doc_index for doc_index, prob in top_documents]

plt.figure(figsize=(10, 6))
for doc_index in top_document_indices:
    plt.plot(W[doc_index], label=f'Doc {doc_index}')

plt.xlabel('Topic')
plt.ylabel('Topic Weight')
plt.title(f'Topic Distribution for Top 10 Documents Related to Topic {most_dominant_topic}')
plt.legend(loc='upper right')
plt.show()

"""## 3.5 ageYoung Topics"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 15
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_age_young = copy.deepcopy(ageYoungProcessedData)
print(f"Age young data samples: {len(processing_data_age_young)}")

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_age_young.values())
print(f"Total number of files in the dataset: {total_files}")

random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_age_young.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot word clouds for the top 2 dominant topics
plt.figure(figsize=(10, 5), dpi=128)

for i, topic_idx in enumerate(top_topic_indices):
    ax = plt.subplot(1, 2, i + 1)
    word_freq = {feature_names[j]: H[topic_idx, j] for j in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Extract the top 10 documents most related to each dominant topic
def get_top_documents(W, topic_num, top_n=10):
    topic_probabilities = [(i, prob) for i, prob in enumerate(W[:, topic_num])]
    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    return top_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top documents
for topic_num in top_topic_indices:
    top_documents = get_top_documents(W, topic_num, top_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

# Plot the topic distribution for the top 10 documents related to the most dominant topic
most_dominant_topic = top_topic_indices[0]
top_documents = get_top_documents(W, most_dominant_topic, top_n=10)
top_document_indices = [doc_index for doc_index, prob in top_documents]

plt.figure(figsize=(10, 6))
for doc_index in top_document_indices:
    plt.plot(W[doc_index], label=f'Doc {doc_index}')

plt.xlabel('Topic')
plt.ylabel('Topic Weight')
plt.title(f'Topic Distribution for Top 10 Documents Related to Topic {most_dominant_topic}')
plt.legend(loc='upper right')
plt.show()

"""## 3.6 Everyone Topics"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 15
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing

processing_data_everyone = copy.deepcopy(everyoneProcessedData)
print(f"Everyone data samples: {len(processing_data_everyone)}")

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_everyone.values())
print(f"Total number of files in the dataset: {total_files}")

random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_everyone.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot word clouds for the top 2 dominant topics
plt.figure(figsize=(10, 5), dpi=128)

for i, topic_idx in enumerate(top_topic_indices):
    ax = plt.subplot(1, 2, i + 1)
    word_freq = {feature_names[j]: H[topic_idx, j] for j in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Extract the top 10 documents most related to each dominant topic
def get_top_documents(W, topic_num, top_n=10):
    topic_probabilities = [(i, prob) for i, prob in enumerate(W[:, topic_num])]
    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    return top_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top documents
for topic_num in top_topic_indices:
    top_documents = get_top_documents(W, topic_num, top_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

# Plot the topic distribution for the top 10 documents related to the most dominant topic
most_dominant_topic = top_topic_indices[0]
top_documents = get_top_documents(W, most_dominant_topic, top_n=10)
top_document_indices = [doc_index for doc_index, prob in top_documents]

plt.figure(figsize=(10, 6))
for doc_index in top_document_indices:
    plt.plot(W[doc_index], label=f'Doc {doc_index}')

plt.xlabel('Topic')
plt.ylabel('Topic Weight')
plt.title(f'Topic Distribution for Top 10 Documents Related to Topic {most_dominant_topic}')
plt.legend(loc='upper right')
plt.show()

"""RUNBEFORE

# Part 4: Topics Detection By Different Methods

## 4.1 Counting all NOUNS
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import spacy
import copy
import re
import random
import time

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Initialize spaCy model for text processing
nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])  # Disable unnecessary components for speed

# Function to preprocess text and extract nouns
def preprocess_and_extract_nouns(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc if token.pos_ == 'NOUN']

# Preprocess the data and extract nouns
processed_nouns = [" ".join(preprocess_and_extract_nouns(" ".join(file))) for file in all_files]

# Create CountVectorizer model
vectorizer = CountVectorizer(max_features=10000, stop_words='english')
X = vectorizer.fit_transform(processed_nouns)

# Get feature names (nouns)
feature_names = vectorizer.get_feature_names_out()

# Sum the counts for each noun across all documents
noun_counts_sum = np.sum(X.toarray(), axis=0)

# Get the indices of the top 20 nouns by count
top_n_indices = np.argsort(noun_counts_sum)[::-1][:20]

# Get the top 20 nouns and their corresponding counts
top_nouns = [(feature_names[i], noun_counts_sum[i]) for i in top_n_indices]

# Print the top 20 nouns and their counts
print("Top 20 nouns across all documents:")
for noun, count in top_nouns:
    print(f"{noun}: {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

"""### 4.1.1 Counting Nouns and Extract Clauses"""

# Function to check if a document contains at least 10 of the top 20 nouns
def contains_top_nouns(doc, top_nouns, threshold=10):
    doc_nouns = set(preprocess_and_extract_nouns(" ".join(doc)))
    top_noun_set = set([noun for noun, _ in top_nouns])
    common_nouns = doc_nouns.intersection(top_noun_set)
    return len(common_nouns) >= threshold, common_nouns

# Iterate through all files and check for the top 20 nouns
docs_with_top_nouns = []

for idx, file in enumerate(all_files):
    contains, common_nouns = contains_top_nouns(file, top_nouns)
    if contains:
        docs_with_top_nouns.append((file, common_nouns))

# Print documents that contain at least 10 of the top 20 nouns
print(f"Number of documents with at least 10 of the top 20 nouns: {len(docs_with_top_nouns)}")
for i, (doc, common_nouns) in enumerate(docs_with_top_nouns):
    print(f"Document {i+1}:")
    print(" ".join(doc))
    print(f"Common nouns: {', '.join(common_nouns)}")
    print("\n")

# Print the summary of the extraction process
print(f"Total number of documents processed: {len(all_files)}")
print(f"Number of documents with at least 10 of the top 20 nouns: {len(docs_with_top_nouns)}")

# Save the results to a text file
with open("documents_with_top_nouns.txt", "w", encoding="utf-8") as f:
    f.write(f"Number of documents with at least 10 of the top 20 nouns: {len(docs_with_top_nouns)}\n")
    for i, (doc, common_nouns) in enumerate(docs_with_top_nouns):
        f.write(f"Document {i+1}:\n")
        f.write(" ".join(doc) + "\n")
        f.write(f"Common nouns: {', '.join(common_nouns)}\n\n")

print("The results have been saved to 'documents_with_top_nouns.txt'")

"""## 4.2 Counting all subjects, direct objects, and prepositional objects


"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import spacy
import copy
import re
import random
import time

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Initialize spaCy model for text processing
nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])  # Disable unnecessary components for speed

# Function to preprocess text and extract subjects, direct objects, and prepositional objects
def preprocess_and_extract_roles(text):
    doc = nlp(text)
    subjects = [token.lemma_ for token in doc if token.dep_ in ('nsubj', 'nsubjpass')]
    direct_objects = [token.lemma_ for token in doc if token.dep_ == 'dobj']
    prepositional_objects = [token.lemma_ for token in doc if token.dep_ == 'pobj']
    return subjects, direct_objects, prepositional_objects

# Preprocess the data and extract subjects, direct objects, and prepositional objects
all_subjects = []
all_direct_objects = []
all_prepositional_objects = []

for file in all_files:
    concatenated_text = " ".join(file)
    subjects, direct_objects, prepositional_objects = preprocess_and_extract_roles(concatenated_text)
    all_subjects.extend(subjects)
    all_direct_objects.extend(direct_objects)
    all_prepositional_objects.extend(prepositional_objects)

# Combine all extracted roles for vectorization
combined_roles = {
    'subjects': all_subjects,
    'direct_objects': all_direct_objects,
    'prepositional_objects': all_prepositional_objects
}

# Function to count top N roles
def count_top_n_roles(roles, top_n=20):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(roles)
    feature_names = vectorizer.get_feature_names_out()
    counts = np.sum(X.toarray(), axis=0)
    top_indices = np.argsort(counts)[::-1][:top_n]
    top_roles = [(feature_names[i], counts[i]) for i in top_indices]
    return top_roles

# Count and print top 20 subjects, direct objects, and prepositional objects
for role_name, roles in combined_roles.items():
    top_roles = count_top_n_roles(roles, top_n=20)
    print(f"Top 20 {role_name.replace('_', ' ')}:")
    for role, count in top_roles:
        print(f"{role}: {count}")
    print("\n")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

"""### 4.2.1 Extract Clauses"""

import spacy
import copy
import random

# Assuming top_roles_dict, all_files, and preprocess_and_extract_roles are defined earlier in the notebook

# Initialize spaCy model for text processing
nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])  # Disable unnecessary components for speed

# Function to preprocess text and extract roles
def preprocess_and_extract_roles(text):
    doc = nlp(text)
    subjects = [token.lemma_ for token in doc if token.dep_ in ('nsubj', 'nsubjpass')]
    direct_objects = [token.lemma_ for token in doc if token.dep_ == 'dobj']
    prepositional_objects = [token.lemma_ for token in doc if token.dep_ == 'pobj']
    return subjects, direct_objects, prepositional_objects

# Function to extract and print documents containing at least N top roles
def extract_documents_with_top_roles(top_roles, role_index, all_files, threshold=10, max_docs=50):
    docs_with_top_roles = []

    for file in all_files:
        concatenated_text = " ".join(file)
        doc_roles = set(preprocess_and_extract_roles(concatenated_text)[role_index])
        top_role_set = set([role for role, _ in top_roles])
        common_roles = doc_roles.intersection(top_role_set)
        if len(common_roles) >= threshold:
            docs_with_top_roles.append((file, common_roles))
        if len(docs_with_top_roles) >= max_docs:
            break

    print(f"\nDocuments with at least {threshold} of the top 20 roles (showing up to {max_docs} documents):")
    for i, (doc, common_roles) in enumerate(docs_with_top_roles):
        print(f"Document {i+1}:")
        print(" ".join(doc))
        print(f"Common roles: {', '.join(common_roles)}")
        print("\n")

    return docs_with_top_roles

# Extract and print documents for each role type
role_indices = {'subjects': 0, 'direct_objects': 1, 'prepositional_objects': 2}
for role_name, top_roles in top_roles_dict.items():
    role_index = role_indices[role_name]
    extract_documents_with_top_roles(top_roles, role_index, all_files, threshold=10, max_docs=50)

"""## 4.3 TF-IDF Method"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import spacy
import copy
import re
import random
import time

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Initialize spaCy model for text processing
nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])  # Disable unnecessary components for speed

# Function to preprocess text
def preprocess_text(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]

# Preprocess the data
processed_data = [" ".join(preprocess_text(" ".join(file))) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Sum the TF-IDF scores for each word across all documents
tfidf_scores_sum = np.sum(X.toarray(), axis=0)

# Get the indices of the top 20 words by TF-IDF score
top_n_indices = np.argsort(tfidf_scores_sum)[::-1][:20]

# Get the top 20 words and their corresponding TF-IDF scores
top_words = [(feature_names[i], tfidf_scores_sum[i]) for i in top_n_indices]

# Print the top 20 words and their TF-IDF scores
print("Top 20 words across all documents:")
for word, score in top_words:
    print(f"{word}: {score:.4f}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

"""### 4.3.1 Extract Clauses"""

import spacy
import random

# Initialize spaCy model for text processing
nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])  # Disable unnecessary components for speed

# Function to preprocess text
def preprocess_text(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]

# Function to extract and print documents containing at least N top words
def extract_documents_with_top_words(top_words, all_files, threshold=10, max_docs=50):
    top_word_set = set([word for word, _ in top_words])
    docs_with_top_words = []

    for file in all_files:
        concatenated_text = " ".join(file)
        doc_words = set(preprocess_text(concatenated_text))
        common_words = doc_words.intersection(top_word_set)
        if len(common_words) >= threshold:
            docs_with_top_words.append((file, common_words))
        if len(docs_with_top_words) >= max_docs:
            break

    print(f"\nDocuments with at least {threshold} of the top 20 words (showing up to {max_docs} documents):")
    for i, (doc, common_words) in enumerate(docs_with_top_words):
        print(f"Document {i+1}:")
        print(" ".join(doc))
        print(f"Common words: {', '.join(common_words)}")
        print("\n")

    return docs_with_top_words

# Assuming top_words and all_files are defined earlier in the notebook

# Extract and print documents containing at least 10 of the top 20 words
docs_with_top_words = extract_documents_with_top_words(top_words, all_files, threshold=10, max_docs=50)

"""## 4.4 nGram Methods"""

import pandas as pd
import numpy as np
from collections import Counter
from nltk import ngrams
import spacy
import copy
import re
import random
import time

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Initialize spaCy model for text processing
nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])  # Disable unnecessary components for speed

# Function to preprocess text
def preprocess_text(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]

# Preprocess the data
processed_data = [" ".join(preprocess_text(' '.join(file))) for file in all_files]

# Function to generate n-grams
def generate_ngrams(input_list, n):
    return list(ngrams(input_list, n))

# Function to count top N n-grams
def count_top_n_ngrams(preprocessed_data, n, top_n=20):
    all_ngrams = []
    for text in preprocessed_data:
        words = text.split()
        n_grams = generate_ngrams(words, n)
        all_ngrams.extend(n_grams)

    ngram_counts = Counter(all_ngrams)
    most_common_ngrams = ngram_counts.most_common(top_n)
    return most_common_ngrams

# Count and print top 20 bigrams
top_bigrams = count_top_n_ngrams(processed_data, n=2)
print("Top 20 common bigrams:")
for bigram, count in top_bigrams:
    print(f"{bigram}: {count}")

# Count and print top 20 trigrams
top_trigrams = count_top_n_ngrams(processed_data, n=3)
print("\n\nTop 20 common trigrams:")
for trigram, count in top_trigrams:
    print(f"{trigram}: {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

"""### 4.4.1 Extract Clauses"""

import spacy
from nltk import ngrams

# Initialize spaCy model for text processing
nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])  # Disable unnecessary components for speed

# Function to preprocess text
def preprocess_text(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]

# Function to generate n-grams
def generate_ngrams(input_list, n):
    return list(ngrams(input_list, n))

# Function to extract and print documents containing at least N top n-grams
def extract_documents_with_top_ngrams(top_ngrams, n, all_files, threshold=10, max_docs=50):
    top_ngram_set = set(ngram for ngram, _ in top_ngrams)
    docs_with_top_ngrams = []

    for file in all_files:
        concatenated_text = " ".join(file)
        doc_words = preprocess_text(concatenated_text)
        doc_ngrams = set(generate_ngrams(doc_words, n))
        common_ngrams = doc_ngrams.intersection(top_ngram_set)
        if len(common_ngrams) >= threshold:
            docs_with_top_ngrams.append((file, common_ngrams))
        if len(docs_with_top_ngrams) >= max_docs:
            break

    print(f"\nDocuments with at least {threshold} of the top 20 {n}-grams (showing up to {max_docs} documents):")
    for i, (doc, common_ngrams) in enumerate(docs_with_top_ngrams):
        print(f"Document {i+1}:")
        print(" ".join(doc))
        print(f"Common {n}-grams: {', '.join([' '.join(ngram) for ngram in common_ngrams])}")
        print("\n")

    return docs_with_top_ngrams

# Assuming top_bigrams, top_trigrams, and all_files are defined earlier in the notebook

# Extract and print documents containing at least 10 of the top 20 bigrams
docs_with_top_bigrams = extract_documents_with_top_ngrams(top_bigrams, n=2, all_files=all_files, threshold=10, max_docs=50)

# Extract and print documents containing at least 10 of the top 20 trigrams
docs_with_top_trigrams = extract_documents_with_top_ngrams(top_trigrams, n=3, all_files=all_files, threshold=5, max_docs=50)

"""## 4.5 LDA with TF-IDF; LDA doesn't work well with TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 20
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data.values())
print(f"Total number of files in the dataset: {total_files}")


random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the LDA model
lda_model = LDA(n_components=num_topics, random_state=random_state)
W = lda_model.fit_transform(X)
H = lda_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Function to display the top words for each topic
def display_topics(H, feature_names, num_top_words):
    for topic_idx, topic in enumerate(H):
        print(f"Topic {topic_idx}:")
        top_indices = topic.argsort()[:-num_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        print(" ".join(top_words))

# Display the top words for each topic
display_topics(H, feature_names, num_top_words)

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(lda_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in lda_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(lda_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {feature_names[i]: H[topic_idx, i] for i in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(lda_model, dictionary, top_topic_indices, top_terms=30):
    topic_term_matrix = lda_model.components_
    vocab = dictionary.token2id

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [dictionary.id2token[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in top_topic_indices:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, vocab[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(lda_model, dictionary, top_topic_indices, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(W, H, texts, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in top_topic_indices[:topn]:
        words = H[topic_num].argsort()[:-10 - 1:-1]
        keywords.extend([feature_names[i] for i in words])

    clauses = {topic_num: [] for topic_num in top_topic_indices[:topn]}
    for i, topic_dist in enumerate(W):
        dominant_topic = topic_dist.argsort()[-1]
        if dominant_topic in top_topic_indices[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(W, H, processed_data, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""# Part 5: LDA With CountiVectorizer
20 Num_Topics get highest coherence score 0.52 with 5000 students files

## 5.1 50 Topics
"""

import pandas as pd
import numpy as np
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from sklearn.feature_extraction.text import CountVectorizer
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_11 = copy.deepcopy(studentProcessedData)

# Set random seed for reproducibility
random.seed(42)

# Parameters for easy adjustment
alpha_param = 'auto'
eta_param = 'auto'
num_topics = 50
num_keywords = 15
no_above = 0.5
no_below = 0.1

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_11.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
all_files = [file for category, files in processing_data_11.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary([text.split() for text in processed_data])

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)

# Create a corpus from the dictionary and processed data
corpus = [dictionary.doc2bow(text.split()) for text in processed_data]

# Build the LDA model with optimized parameters
lda_model = LdaModel(corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=100,
            update_every=1,
            chunksize=100,
            passes=5,  # Reduce the number of passes
            alpha=alpha_param,
            eta=eta_param,
            per_word_topics=True)

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in processed_data], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

# Print the topics with the highest coherence score
topics = lda_model.print_topics(num_words=num_keywords)
for topic in topics:
    print(topic)

# Extract the 2 most dominant topics for the sampled files
def get_dominant_topics_for_dataset(ldamodel, corpus, topn=2):
    topic_counts = np.zeros(ldamodel.num_topics)
    for corp in corpus:
        topic_probs = ldamodel.get_document_topics(corp, minimum_probability=0.0)
        for topic_num, prob in topic_probs:
            topic_counts[topic_num] += prob

    dominant_topics = topic_counts.argsort()[-topn:][::-1]
    return dominant_topics, topic_counts[dominant_topics]

dominant_topics, topic_counts = get_dominant_topics_for_dataset(lda_model, corpus, topn=2)

# Get the topic names
topic_names = []
for topic_num in dominant_topics:
    words = lda_model.show_topic(topic_num, topn=num_keywords)
    topic_name = ", ".join([word for word, prob in words])
    topic_names.append(topic_name)

# Display the 2 dominant topics for the sampled files
print("Top 2 dominant topics in the sampled files:")
for topic_num, topic_name, count in zip(dominant_topics, topic_names, topic_counts):
    print(f"Topic ({topic_name}): Topic {topic_num}, Count {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_keywords,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(lda_model.num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {word: prob for word, prob in lda_model.show_topic(topic_idx, topn=num_keywords)}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_keywords} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot
def draw_termite_plot(lda_model, dictionary, num_topics=10, num_keywords=15):
    topic_term_matrix = lda_model.get_topics()
    vocab = [dictionary[id] for id in range(len(dictionary))]
    top_terms = np.argsort(topic_term_matrix, axis=1)[:, -num_keywords:]
    top_terms = [[vocab[id] for id in topic] for topic in top_terms]

    term_freqs = []
    for topic_idx in range(num_topics):
        for term_idx in range(num_keywords):
            term_freqs.append((topic_idx, top_terms[topic_idx][term_idx], topic_term_matrix[topic_idx, dictionary.token2id[top_terms[topic_idx][term_idx]]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Topic", "Term", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Term", y="Topic", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xticks(rotation=90)
    plt.xlabel("Term")
    plt.ylabel("Topic")
    plt.title("Termite Plot")
    plt.show()

draw_termite_plot(lda_model, dictionary, num_topics=num_topics, num_keywords=num_keywords)

# Extract the clauses containing the dominant topics
def extract_clauses_with_dominant_topics(ldamodel, corpus, texts, dominant_topics):
    clauses = []
    for i, doc in enumerate(corpus):
        topic_probs = ldamodel.get_document_topics(doc, minimum_probability=0.0)
        dominant_topic = sorted(topic_probs, key=lambda x: -x[1])[0][0]
        if dominant_topic in dominant_topics:
            clauses.append(texts[i])
    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(lda_model, corpus, processed_data, dominant_topics)

print("Extracted clauses with dominant topics:")
for clause in clauses_with_dominant_topics[:10]:  # Print the first 10 clauses
    print(clause)

"""## 5.2 Topic = 30"""

import pandas as pd
import numpy as np
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from sklearn.feature_extraction.text import CountVectorizer
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_1 = copy.deepcopy(studentProcessedData)

# Set random seed for reproducibility
random.seed(42)

# Parameters for easy adjustment
alpha_param = 'auto'
eta_param = 'auto'
num_topics = 30
num_keywords = 15
no_above = 0.5
no_below = 0.1

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_1.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
all_files = [file for category, files in processing_data_1.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary([text.split() for text in processed_data])

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)

# Create a corpus from the dictionary and processed data
corpus = [dictionary.doc2bow(text.split()) for text in processed_data]

# Build the LDA model with optimized parameters
lda_model = LdaModel(corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=100,
            update_every=1,
            chunksize=100,
            passes=5,  # Reduce the number of passes
            alpha=alpha_param,
            eta=eta_param,
            per_word_topics=True)

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in processed_data], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

# Print the topics with the highest coherence score
topics = lda_model.print_topics(num_words=num_keywords)
for topic in topics:
    print(topic)

# Extract the 2 most dominant topics for the sampled files
def get_dominant_topics_for_dataset(ldamodel, corpus, topn=2):
    topic_counts = np.zeros(ldamodel.num_topics)
    for corp in corpus:
        topic_probs = ldamodel.get_document_topics(corp, minimum_probability=0.0)
        for topic_num, prob in topic_probs:
            topic_counts[topic_num] += prob

    dominant_topics = topic_counts.argsort()[-topn:][::-1]
    return dominant_topics, topic_counts[dominant_topics]

dominant_topics, topic_counts = get_dominant_topics_for_dataset(lda_model, corpus, topn=2)

# Get the topic names
topic_names = []
for topic_num in dominant_topics:
    words = lda_model.show_topic(topic_num, topn=num_keywords)
    topic_name = ", ".join([word for word, prob in words])
    topic_names.append(topic_name)

# Display the 2 dominant topics for the sampled files
print("Top 2 dominant topics in the sampled files:")
for topic_num, topic_name, count in zip(dominant_topics, topic_names, topic_counts):
    print(f"Topic ({topic_name}): Topic {topic_num}, Count {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_keywords,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(lda_model.num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {word: prob for word, prob in lda_model.show_topic(topic_idx, topn=num_keywords)}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_keywords} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot
def draw_termite_plot(lda_model, dictionary, num_topics=10, num_keywords=15):
    topic_term_matrix = lda_model.get_topics()
    vocab = [dictionary[id] for id in range(len(dictionary))]
    top_terms = np.argsort(topic_term_matrix, axis=1)[:, -num_keywords:]
    top_terms = [[vocab[id] for id in topic] for topic in top_terms]

    term_freqs = []
    for topic_idx in range(num_topics):
        for term_idx in range(num_keywords):
            term_freqs.append((topic_idx, top_terms[topic_idx][term_idx], topic_term_matrix[topic_idx, dictionary.token2id[top_terms[topic_idx][term_idx]]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Topic", "Term", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Term", y="Topic", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xticks(rotation=90)
    plt.xlabel("Term")
    plt.ylabel("Topic")
    plt.title("Termite Plot")
    plt.show()

draw_termite_plot(lda_model, dictionary, num_topics=num_topics, num_keywords=num_keywords)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = [sentence for sentence in sentences if any(keyword in sentence for keyword in keywords)]
    return extracted_sentences

def extract_clauses_with_dominant_topics(ldamodel, corpus, texts, dominant_topics, topn=2, max_clauses=30):
    keywords = []
    for topic_num in dominant_topics[:topn]:
        words = ldamodel.show_topic(topic_num, topn=10)
        keywords.extend([word for word, prob in words])

    clauses = {topic_num: [] for topic_num in dominant_topics[:topn]}
    for i, doc in enumerate(corpus):
        topic_probs = ldamodel.get_document_topics(doc, minimum_probability=0.0)
        dominant_topic = sorted(topic_probs, key=lambda x: -x[1])[0][0]
        if dominant_topic in dominant_topics[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords)
            clauses[dominant_topic].extend(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(lda_model, corpus, processed_data, dominant_topics, topn=2, max_clauses=30)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""## 5.3 * LDA Num_Topic = 20 -> coherence 0.52"""

import pandas as pd
import numpy as np
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 20
num_top_words = 15
no_above = 0.5
no_below = 0.1
random_state = 42
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_1 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_1.values())
print(f"Total number of files in the dataset: {total_files}")

random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_1.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary([text.split() for text in processed_data])

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)

# Create a corpus from the dictionary and processed data
corpus = [dictionary.doc2bow(text.split()) for text in processed_data]

# Build the LDA model with optimized parameters
lda_model = LdaModel(corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=100,
            update_every=1,
            chunksize=100,
            passes=5,
            alpha='auto',
            eta='auto',
            per_word_topics=True)

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in processed_data], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

# Print the topics with the highest coherence score
topics = lda_model.print_topics(num_words=num_top_words)
for topic in topics:
    print(topic)

# Extract the 2 most dominant topics for the sampled files
def get_dominant_topics_for_dataset(ldamodel, corpus, topn=2):
    topic_counts = np.zeros(ldamodel.num_topics)
    for corp in corpus:
        topic_probs = ldamodel.get_document_topics(corp, minimum_probability=0.0)
        for topic_num, prob in topic_probs:
            topic_counts[topic_num] += prob

    dominant_topics = topic_counts.argsort()[-topn:][::-1]
    return dominant_topics, topic_counts[dominant_topics]

dominant_topics, topic_counts = get_dominant_topics_for_dataset(lda_model, corpus, topn=2)

# Get the topic names
topic_names = []
for topic_num in dominant_topics:
    words = lda_model.show_topic(topic_num, topn=num_top_words)
    topic_name = ", ".join([word for word, prob in words])
    topic_names.append(topic_name)

# Display the 2 dominant topics for the sampled files
print("Top 2 dominant topics in the sampled files:")
for topic_num, topic_name, count in zip(dominant_topics, topic_names, topic_counts):
    print(f"Topic ({topic_name}): Topic {topic_num}, Count {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(lda_model.num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {word: prob for word, prob in lda_model.show_topic(topic_idx, num_top_words)}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=30):
    topic_term_matrix = lda_model.get_topics()
    vocab = [dictionary[id] for id in range(len(dictionary))]

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [vocab[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in dominant_topics:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, dictionary.token2id[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=termite_top_terms)

# Extract the documents with the top 10 and median 10 probabilities for the dominant topics
def get_documents_with_top_and_median_probabilities(ldamodel, corpus, topic_num, top_n=10, median_n=10):
    topic_probabilities = []
    for i, doc in enumerate(corpus):
        topic_probs = ldamodel.get_document_topics(doc, minimum_probability=0.0)
        for topic, prob in topic_probs:
            if topic == topic_num:
                topic_probabilities.append((i, prob))

    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    median_documents = topic_probabilities[len(topic_probabilities)//2 - median_n//2:len(topic_probabilities)//2 + median_n//2]

    return top_documents, median_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top and median documents
for topic_num in dominant_topics:
    top_documents, median_documents = get_documents_with_top_and_median_probabilities(lda_model, corpus, topic_num, top_n=10, median_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

    print(f"\nMedian 10 documents for Topic {topic_num}:")
    median_sentences = extract_sentences_from_documents(median_documents, processed_data, max_clause_length)
    for sentence in median_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)
    print("\n")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

"""## 5.4 Num_Topic = 40"""

import pandas as pd
import numpy as np
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_1 = copy.deepcopy(studentProcessedData)

# Set random seed for reproducibility
random.seed(42)

# Parameters for easy adjustment
alpha_param = 'auto'
eta_param = 'auto'
num_topics = 40
num_keywords = 15
no_above = 0.5
no_below = 0.1
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_1.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
all_files = [file for category, files in processing_data_1.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary([text.split() for text in processed_data])

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)

# Create a corpus from the dictionary and processed data
corpus = [dictionary.doc2bow(text.split()) for text in processed_data]

# Build the LDA model with optimized parameters
lda_model = LdaModel(corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=100,
            update_every=1,
            chunksize=100,
            passes=5,  # Reduce the number of passes
            alpha=alpha_param,
            eta=eta_param,
            per_word_topics=True)

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in processed_data], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

# Print the topics with the highest coherence score
topics = lda_model.print_topics(num_words=num_keywords)
for topic in topics:
    print(topic)

# Extract the 2 most dominant topics for the sampled files
def get_dominant_topics_for_dataset(ldamodel, corpus, topn=2):
    topic_counts = np.zeros(ldamodel.num_topics)
    for corp in corpus:
        topic_probs = ldamodel.get_document_topics(corp, minimum_probability=0.0)
        for topic_num, prob in topic_probs:
            topic_counts[topic_num] += prob

    dominant_topics = topic_counts.argsort()[-topn:][::-1]
    return dominant_topics, topic_counts[dominant_topics]

dominant_topics, topic_counts = get_dominant_topics_for_dataset(lda_model, corpus, topn=2)

# Get the topic names
topic_names = []
for topic_num in dominant_topics:
    words = lda_model.show_topic(topic_num, topn=num_keywords)
    topic_name = ", ".join([word for word, prob in words])
    topic_names.append(topic_name)

# Display the 2 dominant topics for the sampled files
print("Top 2 dominant topics in the sampled files:")
for topic_num, topic_name, count in zip(dominant_topics, topic_names, topic_counts):
    print(f"Topic ({topic_name}): Topic {topic_num}, Count {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_keywords,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(lda_model.num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {word: prob for word, prob in lda_model.show_topic(topic_idx, topn=num_keywords)}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_keywords} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=30):
    topic_term_matrix = lda_model.get_topics()
    vocab = [dictionary[id] for id in range(len(dictionary))]

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [vocab[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in dominant_topics:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, dictionary.token2id[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(ldamodel, corpus, texts, dominant_topics, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in dominant_topics[:topn]:
        words = ldamodel.show_topic(topic_num, topn=10)
        keywords.extend([word for word, prob in words])

    clauses = {topic_num: [] for topic_num in dominant_topics[:topn]}
    for i, doc in enumerate(corpus):
        topic_probs = ldamodel.get_document_topics(doc, minimum_probability=0.0)
        dominant_topic = sorted(topic_probs, key=lambda x: -x[1])[0][0]
        if dominant_topic in dominant_topics[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(lda_model, corpus, processed_data, dominant_topics, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""## 5.5 Num_topic = 60"""

import pandas as pd
import numpy as np
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_1 = copy.deepcopy(studentProcessedData)

# Set random seed for reproducibility
random.seed(42)

# Parameters for easy adjustment
alpha_param = 'auto'
eta_param = 'auto'
num_topics = 60
num_keywords = 15
no_above = 0.5
no_below = 0.1
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_1.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
all_files = [file for category, files in processing_data_1.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary([text.split() for text in processed_data])

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)

# Create a corpus from the dictionary and processed data
corpus = [dictionary.doc2bow(text.split()) for text in processed_data]

# Build the LDA model with optimized parameters
lda_model = LdaModel(corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=100,
            update_every=1,
            chunksize=100,
            passes=5,  # Reduce the number of passes
            alpha=alpha_param,
            eta=eta_param,
            per_word_topics=True)

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in processed_data], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

# Print the topics with the highest coherence score
topics = lda_model.print_topics(num_words=num_keywords)
for topic in topics:
    print(topic)

# Extract the 2 most dominant topics for the sampled files
def get_dominant_topics_for_dataset(ldamodel, corpus, topn=2):
    topic_counts = np.zeros(ldamodel.num_topics)
    for corp in corpus:
        topic_probs = ldamodel.get_document_topics(corp, minimum_probability=0.0)
        for topic_num, prob in topic_probs:
            topic_counts[topic_num] += prob

    dominant_topics = topic_counts.argsort()[-topn:][::-1]
    return dominant_topics, topic_counts[dominant_topics]

dominant_topics, topic_counts = get_dominant_topics_for_dataset(lda_model, corpus, topn=2)

# Get the topic names
topic_names = []
for topic_num in dominant_topics:
    words = lda_model.show_topic(topic_num, topn=num_keywords)
    topic_name = ", ".join([word for word, prob in words])
    topic_names.append(topic_name)

# Display the 2 dominant topics for the sampled files
print("Top 2 dominant topics in the sampled files:")
for topic_num, topic_name, count in zip(dominant_topics, topic_names, topic_counts):
    print(f"Topic ({topic_name}): Topic {topic_num}, Count {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_keywords,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(lda_model.num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {word: prob for word, prob in lda_model.show_topic(topic_idx, topn=num_keywords)}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_keywords} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=30):
    topic_term_matrix = lda_model.get_topics()
    vocab = [dictionary[id] for id in range(len(dictionary))]

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [vocab[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in dominant_topics:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, dictionary.token2id[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(ldamodel, corpus, texts, dominant_topics, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in dominant_topics[:topn]:
        words = ldamodel.show_topic(topic_num, topn=10)
        keywords.extend([word for word, prob in words])

    clauses = {topic_num: [] for topic_num in dominant_topics[:topn]}
    for i, doc in enumerate(corpus):
        topic_probs = ldamodel.get_document_topics(doc, minimum_probability=0.0)
        dominant_topic = sorted(topic_probs, key=lambda x: -x[1])[0][0]
        if dominant_topic in dominant_topics[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(lda_model, corpus, processed_data, dominant_topics, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""## 5.6 Num_topic = 10"""

import pandas as pd
import numpy as np
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_1 = copy.deepcopy(studentProcessedData)

# Set random seed for reproducibility
random.seed(42)

# Parameters for easy adjustment
alpha_param = 'auto'
eta_param = 'auto'
num_topics = 10
num_keywords = 15
no_above = 0.5
no_below = 0.1
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_1.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
all_files = [file for category, files in processing_data_1.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary([text.split() for text in processed_data])

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)

# Create a corpus from the dictionary and processed data
corpus = [dictionary.doc2bow(text.split()) for text in processed_data]

# Build the LDA model with optimized parameters
lda_model = LdaModel(corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=100,
            update_every=1,
            chunksize=100,
            passes=5,  # Reduce the number of passes
            alpha=alpha_param,
            eta=eta_param,
            per_word_topics=True)

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in processed_data], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

# Print the topics with the highest coherence score
topics = lda_model.print_topics(num_words=num_keywords)
for topic in topics:
    print(topic)

# Extract the 2 most dominant topics for the sampled files
def get_dominant_topics_for_dataset(ldamodel, corpus, topn=2):
    topic_counts = np.zeros(ldamodel.num_topics)
    for corp in corpus:
        topic_probs = ldamodel.get_document_topics(corp, minimum_probability=0.0)
        for topic_num, prob in topic_probs:
            topic_counts[topic_num] += prob

    dominant_topics = topic_counts.argsort()[-topn:][::-1]
    return dominant_topics, topic_counts[dominant_topics]

dominant_topics, topic_counts = get_dominant_topics_for_dataset(lda_model, corpus, topn=2)

# Get the topic names
topic_names = []
for topic_num in dominant_topics:
    words = lda_model.show_topic(topic_num, topn=num_keywords)
    topic_name = ", ".join([word for word, prob in words])
    topic_names.append(topic_name)

# Display the 2 dominant topics for the sampled files
print("Top 2 dominant topics in the sampled files:")
for topic_num, topic_name, count in zip(dominant_topics, topic_names, topic_counts):
    print(f"Topic ({topic_name}): Topic {topic_num}, Count {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_keywords,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(lda_model.num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {word: prob for word, prob in lda_model.show_topic(topic_idx, topn=num_keywords)}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_keywords} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=30):
    topic_term_matrix = lda_model.get_topics()
    vocab = [dictionary[id] for id in range(len(dictionary))]

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [vocab[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in dominant_topics:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, dictionary.token2id[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(ldamodel, corpus, texts, dominant_topics, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in dominant_topics[:topn]:
        words = ldamodel.show_topic(topic_num, topn=10)
        keywords.extend([word for word, prob in words])

    clauses = {topic_num: [] for topic_num in dominant_topics[:topn]}
    for i, doc in enumerate(corpus):
        topic_probs = ldamodel.get_document_topics(doc, minimum_probability=0.0)
        dominant_topic = sorted(topic_probs, key=lambda x: -x[1])[0][0]
        if dominant_topic in dominant_topics[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(lda_model, corpus, processed_data, dominant_topics, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""## 5.7 Num_topic = 15"""

import pandas as pd
import numpy as np
import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_1 = copy.deepcopy(studentProcessedData)

# Set random seed for reproducibility
random.seed(42)

# Parameters for easy adjustment
alpha_param = 'auto'
eta_param = 'auto'
num_topics = 15
num_keywords = 15
no_above = 0.5
no_below = 0.1
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_1.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
all_files = [file for category, files in processing_data_1.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary([text.split() for text in processed_data])

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)

# Create a corpus from the dictionary and processed data
corpus = [dictionary.doc2bow(text.split()) for text in processed_data]

# Build the LDA model with optimized parameters
lda_model = LdaModel(corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=100,
            update_every=1,
            chunksize=100,
            passes=5,  # Reduce the number of passes
            alpha=alpha_param,
            eta=eta_param,
            per_word_topics=True)

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=[text.split() for text in processed_data], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)

# Print the topics with the highest coherence score
topics = lda_model.print_topics(num_words=num_keywords)
for topic in topics:
    print(topic)

# Extract the 2 most dominant topics for the sampled files
def get_dominant_topics_for_dataset(ldamodel, corpus, topn=2):
    topic_counts = np.zeros(ldamodel.num_topics)
    for corp in corpus:
        topic_probs = ldamodel.get_document_topics(corp, minimum_probability=0.0)
        for topic_num, prob in topic_probs:
            topic_counts[topic_num] += prob

    dominant_topics = topic_counts.argsort()[-topn:][::-1]
    return dominant_topics, topic_counts[dominant_topics]

dominant_topics, topic_counts = get_dominant_topics_for_dataset(lda_model, corpus, topn=2)

# Get the topic names
topic_names = []
for topic_num in dominant_topics:
    words = lda_model.show_topic(topic_num, topn=num_keywords)
    topic_name = ", ".join([word for word, prob in words])
    topic_names.append(topic_name)

# Display the 2 dominant topics for the sampled files
print("Top 2 dominant topics in the sampled files:")
for topic_num, topic_name, count in zip(dominant_topics, topic_names, topic_counts):
    print(f"Topic ({topic_name}): Topic {topic_num}, Count {count}")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_keywords,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(lda_model.num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {word: prob for word, prob in lda_model.show_topic(topic_idx, topn=num_keywords)}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_keywords} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=30):
    topic_term_matrix = lda_model.get_topics()
    vocab = [dictionary[id] for id in range(len(dictionary))]

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [vocab[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in dominant_topics:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, dictionary.token2id[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(lda_model, dictionary, dominant_topics, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(ldamodel, corpus, texts, dominant_topics, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in dominant_topics[:topn]:
        words = ldamodel.show_topic(topic_num, topn=10)
        keywords.extend([word for word, prob in words])

    clauses = {topic_num: [] for topic_num in dominant_topics[:topn]}
    for i, doc in enumerate(corpus):
        topic_probs = ldamodel.get_document_topics(doc, minimum_probability=0.0)
        dominant_topic = sorted(topic_probs, key=lambda x: -x[1])[0][0]
        if dominant_topic in dominant_topics[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(lda_model, corpus, processed_data, dominant_topics, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""# Part 6: NMF With TFIDFVectorizer
15 Num_Topics get highest coherence score 0.59 with 5000 students files

## 6.1 Num_Topic = 20
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 20
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Function to display the top words for each topic
def display_topics(H, feature_names, num_top_words):
    for topic_idx, topic in enumerate(H):
        print(f"Topic {topic_idx}:")
        top_indices = topic.argsort()[:-num_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        print(" ".join(top_words))

# Display the top words for each topic
display_topics(H, feature_names, num_top_words)

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {feature_names[i]: H[topic_idx, i] for i in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=30):
    topic_term_matrix = nmf_model.components_
    vocab = dictionary.token2id

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [dictionary.id2token[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in top_topic_indices:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, vocab[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(W, H, texts, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in top_topic_indices[:topn]:
        words = H[topic_num].argsort()[:-10 - 1:-1]
        keywords.extend([feature_names[i] for i in words])

    clauses = {topic_num: [] for topic_num in top_topic_indices[:topn]}
    for i, topic_dist in enumerate(W):
        dominant_topic = topic_dist.argsort()[-1]
        if dominant_topic in top_topic_indices[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(W, H, processed_data, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""## 6.2 Num_Topic = 40"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 40
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Function to display the top words for each topic
def display_topics(H, feature_names, num_top_words):
    for topic_idx, topic in enumerate(H):
        print(f"Topic {topic_idx}:")
        top_indices = topic.argsort()[:-num_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        print(" ".join(top_words))

# Display the top words for each topic
display_topics(H, feature_names, num_top_words)

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {feature_names[i]: H[topic_idx, i] for i in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=30):
    topic_term_matrix = nmf_model.components_
    vocab = dictionary.token2id

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [dictionary.id2token[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in top_topic_indices:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, vocab[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(W, H, texts, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in top_topic_indices[:topn]:
        words = H[topic_num].argsort()[:-10 - 1:-1]
        keywords.extend([feature_names[i] for i in words])

    clauses = {topic_num: [] for topic_num in top_topic_indices[:topn]}
    for i, topic_dist in enumerate(W):
        dominant_topic = topic_dist.argsort()[-1]
        if dominant_topic in top_topic_indices[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(W, H, processed_data, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")

"""## 6.3*  NMF Num_Topic = 15 -> 0.59 coherence"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 15
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Function to display the top words for each topic
def display_topics(H, feature_names, num_top_words):
    for topic_idx, topic in enumerate(H):
        print(f"Topic {topic_idx}:")
        top_indices = topic.argsort()[:-num_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        print(" ".join(top_words))

# Display the top words for each topic
display_topics(H, feature_names, num_top_words)

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {feature_names[i]: H[topic_idx, i] for i in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=30):
    topic_term_matrix = nmf_model.components_
    vocab = dictionary.token2id

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [dictionary.id2token[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in top_topic_indices:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, vocab[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=termite_top_terms)

# Extract the documents with the top 10 and median 10 probabilities for the dominant topics
def get_documents_with_top_and_median_probabilities(W, topic_num, top_n=10, median_n=10):
    topic_probabilities = [(i, prob) for i, prob in enumerate(W[:, topic_num])]
    topic_probabilities.sort(key=lambda x: x[1], reverse=True)
    top_documents = topic_probabilities[:top_n]
    median_documents = topic_probabilities[len(topic_probabilities)//2 - median_n//2:len(topic_probabilities)//2 + median_n//2]

    return top_documents, median_documents

def extract_sentences_from_documents(documents, texts, max_length):
    extracted_sentences = []
    for doc_index, prob in documents:
        text = texts[doc_index]
        sentences = re.split(r'(?<=[.!?]) +', text)
        extracted_text = []
        total_length = 0
        for sentence in sentences:
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_text.append(sentence)
            if total_length >= max_length and len(extracted_text) == 0:  # Add at least one sentence if none added yet
                extracted_text.append(sentence)
        if extracted_text:
            extracted_sentences.append(' '.join(extracted_text))
    return extracted_sentences

# Iterate over the top topics and extract sentences from the top and median documents
for topic_num in top_topic_indices:
    top_documents, median_documents = get_documents_with_top_and_median_probabilities(W, topic_num, top_n=10, median_n=10)

    print(f"Top 10 documents for Topic {topic_num}:")
    top_sentences = extract_sentences_from_documents(top_documents, processed_data, max_clause_length)
    for sentence in top_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)

    print(f"\nMedian 10 documents for Topic {topic_num}:")
    median_sentences = extract_sentences_from_documents(median_documents, processed_data, max_clause_length)
    for sentence in median_sentences:
        if sentence.strip():  # Ensure the sentence is not empty
            print(sentence)
    print("\n")

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

"""## 6.4 Num_Topic = 10"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import numpy as np
import gensim
from gensim.models.coherencemodel import CoherenceModel
import spacy
import copy
import re
import random
import time
from wordcloud import WordCloud
from matplotlib import colors
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for easy adjustment
num_topics = 10
num_top_words = 15
max_features = 100000
max_df = 0.5
min_df = 0.1
random_state = 42
max_clause_length = 1000
termite_top_terms = 30  # Number of top terms to display in the termite plot

# Assuming studentProcessedData is defined earlier in the notebook
# Copy the preprocessed data for further processing
processing_data_2 = copy.deepcopy(studentProcessedData)

# Record the start time
start_time = time.time()

total_files = sum(len(files) for files in processing_data_2.values())
print(f"Total number of files in the dataset: {total_files}")

# Randomly sample 500 files
random.seed(42)  # For reproducibility
all_files = [file for category, files in processing_data_2.items() for file in files]

for idx in range(len(all_files) - 1, -1, -1):
    # Concatenate all sentences in the sublist into a single string
    concatenated_string = ' '.join(all_files[idx])
    # Calculate the number of characters in the concatenated string
    char_count = len(concatenated_string)
    # Check if the character count is greater than 100,000
    if char_count > 100000:
        # Remove the sublist using pop() and print the removal information
        all_files.pop(idx)

print(len(all_files))

# Use the already pre-processed data
processed_data = [" ".join(file) for file in all_files]

# Create TF-IDF model
vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, stop_words='english')
X = vectorizer.fit_transform(processed_data)

# Fit the NMF model
nmf_model = NMF(n_components=num_topics, random_state=random_state)
W = nmf_model.fit_transform(X)
H = nmf_model.components_

# Get feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Function to display the top words for each topic
def display_topics(H, feature_names, num_top_words):
    for topic_idx, topic in enumerate(H):
        print(f"Topic {topic_idx}:")
        top_indices = topic.argsort()[:-num_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        print(" ".join(top_words))

# Display the top words for each topic
display_topics(H, feature_names, num_top_words)

# Find and print the top 2 topics
def find_top_topics(W, num_top_topics=2):
    topic_totals = np.sum(W, axis=0)
    top_topic_indices = topic_totals.argsort()[-num_top_topics:][::-1]
    return top_topic_indices

# Get the top 2 topics
top_topic_indices = find_top_topics(W, num_top_topics=2)

print("\nTop 2 dominant topics:")
for idx in top_topic_indices:
    print(f"Topic {idx}:")
    top_indices = H[idx].argsort()[:-num_top_words - 1:-1]
    top_words = [feature_names[i] for i in top_indices]
    print(" ".join(top_words))

# Compute coherence score
texts = [text.split() for text in processed_data]
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Function to compute coherence score
def compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary):
    topics = [[feature_names[word_id] for word_id in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]
    coherence_model = CoherenceModel(topics=topics, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

coherence_score = compute_coherence_score(nmf_model, feature_names, texts, corpus, dictionary)
print(f'Coherence Score: {coherence_score}')

# Record the end time
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Time taken to run: {elapsed_time:.2f} seconds")

# Define a function to generate random colors
def randomcolor():
    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']
    color = "#" + ''.join([random.choice(colorArr) for _ in range(6)])
    return color

# Function to generate a word cloud from a tuple of words and their frequencies
def generate_wordcloud(words_freq):
    color_list = [randomcolor() for _ in range(10)]  # Generate 10 random colors
    wordcloud = WordCloud(
        background_color='white',
        max_words=num_top_words,
        max_font_size=50,
        random_state=42,
        colormap=colors.ListedColormap(color_list)
    ).generate_from_frequencies(words_freq)
    return wordcloud

# Plot multiple word clouds for each topic
dis_cols = 2  # Number of columns in the plot grid
dis_rows = int(np.ceil(num_topics / dis_cols))  # Number of rows in the plot grid
plt.figure(figsize=(5 * dis_cols, 5 * dis_rows), dpi=128)

for topic_idx in range(num_topics):
    ax = plt.subplot(dis_rows, dis_cols, topic_idx + 1)
    word_freq = {feature_names[i]: H[topic_idx, i] for i in H[topic_idx].argsort()[:-num_top_words - 1:-1]}
    ax.imshow(generate_wordcloud(word_freq), interpolation="bilinear")
    ax.axis('off')
    ax.set_title(f"Topic {topic_idx} Top {num_top_words} Words", fontsize=15)

plt.tight_layout()
plt.show()

# Draw Termite Plot for two dominant topics
def draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=30):
    topic_term_matrix = nmf_model.components_
    vocab = dictionary.token2id

    # Get global top 30 frequent terms
    term_frequencies = np.sum(topic_term_matrix, axis=0)
    top_global_terms = np.argsort(term_frequencies)[-top_terms:]
    top_global_terms = [dictionary.id2token[id] for id in top_global_terms]

    term_freqs = []
    for term in top_global_terms:
        for topic_idx in top_topic_indices:
            term_freqs.append((term, topic_idx, topic_term_matrix[topic_idx, vocab[term]]))

    termite_df = pd.DataFrame(term_freqs, columns=["Term", "Topic", "Frequency"])

    plt.figure(figsize=(15, 10))
    sns.scatterplot(data=termite_df, x="Topic", y="Term", size="Frequency", hue="Frequency", palette="viridis", sizes=(50, 500), legend=None)
    plt.xlabel("Topic")
    plt.ylabel("Term")
    plt.title("Termite Plot for Dominant Topics")
    plt.show()

draw_termite_plot(nmf_model, dictionary, top_topic_indices, top_terms=termite_top_terms)

# Extract the clauses containing the dominant topics
def extract_sentences_with_keywords(text, keywords, max_length):
    sentences = re.split(r'(?<=[.!?]) +', text)
    extracted_sentences = []
    total_length = 0
    for sentence in sentences:
        if any(keyword in sentence for keyword in keywords):
            total_length += len(sentence.split())
            if total_length <= max_length:
                extracted_sentences.append(sentence)
            else:
                break
    return ' '.join(extracted_sentences)

def extract_clauses_with_dominant_topics(W, H, texts, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=1000):
    keywords = []
    for topic_num in top_topic_indices[:topn]:
        words = H[topic_num].argsort()[:-10 - 1:-1]
        keywords.extend([feature_names[i] for i in words])

    clauses = {topic_num: [] for topic_num in top_topic_indices[:topn]}
    for i, topic_dist in enumerate(W):
        dominant_topic = topic_dist.argsort()[-1]
        if dominant_topic in top_topic_indices[:topn]:
            sentences = extract_sentences_with_keywords(texts[i], keywords, max_length)
            if sentences:
                clauses[dominant_topic].append(sentences)

    # Limit the number of clauses for each topic
    for topic_num in clauses:
        clauses[topic_num] = clauses[topic_num][:max_clauses]

    return clauses

# Extract clauses containing the dominant topics
clauses_with_dominant_topics = extract_clauses_with_dominant_topics(W, H, processed_data, top_topic_indices, feature_names, topn=2, max_clauses=30, max_length=max_clause_length)

# Print extracted clauses for each dominant topic
for topic_num, clauses in clauses_with_dominant_topics.items():
    print(f"Extracted clauses for Topic {topic_num}:")
    for clause in clauses:
        print(clause)
    print("\n")